# Prepare MySQL Aurora DB | Delta Lake | Flights Data

This project demonstrates how to prepare and process flight datasets using **MySQL Aurora DB** for storage, **PySpark** for ETL, and **Delta Lake** on Amazon S3 for scalable analytics.  

---

## ğŸ“Š Architecture
![Architecture](architecture/Screenshot%20from%202025-08-30%2014-51-27.png)

---

## ğŸ“‚ Project Structure

Prepare MySql Aurora DB | Delta Lake | Flights Data
â”‚

â”œâ”€â”€ architecture

â”‚   â”œâ”€â”€ glue_pyspark.pdf                  # Detailed project documentation

â”‚   â””â”€â”€ Screenshot...png                  # Architecture diagram

â”‚

â”œâ”€â”€ flights-data

â”‚   â”œâ”€â”€ flight_logistics.csv              # Logistics dataset

â”‚   â”œâ”€â”€ flights.csv                       # Flight details dataset

â”‚   â””â”€â”€ passenger_experience.csv          # Passenger experience dataset

â”‚

â””â”€â”€ mysql-queries.sql                     # SQL queries for Aurora DB setup

---

## âš™ï¸ Tech Stack
- **Amazon Aurora MySQL** â€“ Source database  
- **PySpark (AWS Glue)** â€“ ETL processing  
- **Amazon S3** â€“ Data lake storage  
- **Delta Lake** â€“ Transactional storage layer on S3  

---

## ğŸš€ Steps to Create a Delta Lake inside Amazon S3

1. **Prepare MySQL Aurora DB**
   - Run the queries in `mysql-queries.sql` to create schema and load flight data.

2. **Extract Data using AWS Glue / PySpark**
   - Create a Glue job (or Spark job) to extract data from Aurora into DataFrames.

   ```python
   df_flights = spark.read \
       .format("jdbc") \
       .option("url", "jdbc:mysql://<aurora-endpoint>:3306/<db_name>") \
       .option("user", "<username>") \
       .option("password", "<password>") \
       .option("dbtable", "flights") \
       .load()

